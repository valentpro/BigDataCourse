{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7435b4-1d24-4f7d-82de-162199d05ea2",
   "metadata": {},
   "source": [
    "# Arrow, Pandas, Polars and PySpark - simple Comparison\n",
    "In this notebook, we want to take a closer look at the concepts and learn which one is best suited for which scenario.\n",
    "Credits:\n",
    "- https://amanjaiswalofficial.medium.com/apache-arrow-making-spark-even-faster-3ae-8ca8e1a67dc7\n",
    "- https://www.datacamp.com/de/tutorial/apache-arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc48ca4-9383-4c6d-8839-596b4f46e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import time\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "RUN_SPARK = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b265923-dccc-4822-b2d7-ae185d953161",
   "metadata": {},
   "source": [
    "## Generate Test Data\n",
    "### lets first generate some test data\n",
    "you can always adjust the size by adjusting `NUM_ROWS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1769b-5e97-44e1-a328-50be380b6a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_ROWS = 10000000 # kann angepasst werden\n",
    "\n",
    "# %%\n",
    "# Datensatz erzeugen\n",
    "np.random.seed(42)\n",
    "id = np.arange(1, NUM_ROWS+1)\n",
    "x = np.random.randn(NUM_ROWS) * 10 + 50\n",
    "y = np.random.randint(1, 5, size=NUM_ROWS)\n",
    "target = 2*x + y + np.random.randn(NUM_ROWS)*5\n",
    "\n",
    "df_pd = pd.DataFrame({\n",
    "    'id': id,\n",
    "    'x': x,\n",
    "    'category': y,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "df_pd.to_csv('data/large_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30035cd2-52b0-4752-884b-92bfdb237afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>x</th><th>category</th><th>target</th></tr><tr><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>54.967142</td><td>2</td><td>121.51567</td></tr><tr><td>2</td><td>48.617357</td><td>1</td><td>89.778238</td></tr><tr><td>3</td><td>56.476885</td><td>3</td><td>108.760504</td></tr><tr><td>4</td><td>65.230299</td><td>3</td><td>126.704923</td></tr><tr><td>5</td><td>47.658466</td><td>3</td><td>101.613006</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────┬───────────┬──────────┬────────────┐\n",
       "│ id  ┆ x         ┆ category ┆ target     │\n",
       "│ --- ┆ ---       ┆ ---      ┆ ---        │\n",
       "│ i64 ┆ f64       ┆ i64      ┆ f64        │\n",
       "╞═════╪═══════════╪══════════╪════════════╡\n",
       "│ 1   ┆ 54.967142 ┆ 2        ┆ 121.51567  │\n",
       "│ 2   ┆ 48.617357 ┆ 1        ┆ 89.778238  │\n",
       "│ 3   ┆ 56.476885 ┆ 3        ┆ 108.760504 │\n",
       "│ 4   ┆ 65.230299 ┆ 3        ┆ 126.704923 │\n",
       "│ 5   ┆ 47.658466 ┆ 3        ┆ 101.613006 │\n",
       "└─────┴───────────┴──────────┴────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpdf = pl.read_csv('large_dataset.csv')\n",
    "tmpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58ea892-4b50-4cb9-ad2c-c1b1bea713e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer\n",
    "def timed(name, func):\n",
    "    start = perf_counter()\n",
    "    result = func()\n",
    "    end = perf_counter()\n",
    "    print(f\"{name}: {end-start:.4f} s\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18000c-6fde-4f5d-bb8a-b0e65d2931cb",
   "metadata": {},
   "source": [
    "## Environment & Spark session setup\n",
    "\n",
    "This cell creates the SparkSession and sets runtime configuration for local execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fee2852-be6a-438a-8ab1-bc0f131eb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Runtime_Comparison\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a214d-573c-43cb-ba38-d96cea3dca63",
   "metadata": {},
   "source": [
    "## Arrow\n",
    "\n",
    "\"Apache Arrow is a multi-language toolbox for building high performance applications that process and transport large data sets. It is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another.\n",
    "\n",
    "A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more\". [https://arrow.apache.org/overview/]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e67979af-2ad5-40d5-bbd5-a5ed9e801251",
   "metadata": {},
   "source": [
    "### Why Arrow\n",
    "- Arrow uses its in-memory format, which has memory buffers organized in columns and batches. \n",
    "- this makes vectorised processing possible, performing operations on entire columns efficiently\n",
    "\n",
    "### Shared Memory Model\n",
    "- Arrow avoids traditional serialization\n",
    "- Instead, it relies on a shared memory model in which multiple processes can directly access the same data without copying or converting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ba7e7-e315-4c94-9cc0-81f8d8b7d974",
   "metadata": {},
   "source": [
    "### Pandas <-> Spark: conversion runtime comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7925ad60-baaa-4969-a5c9-4eb171952fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv(\"large_dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8e1cfb-760f-419a-b9b5-c588c8b0096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Arrow to pdf:: 32.2962 s\n",
      "Without Arrow to spark df:: 181.2396 s\n"
     ]
    }
   ],
   "source": [
    "pandas_df = timed(\"Without Arrow to pdf:\", lambda:spark_df.toPandas())\n",
    "spark_df = timed(\"Without Arrow to spark df:\", lambda:spark.createDataFrame(pandas_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c61293-9311-40c5-a6e0-3552e426ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Arrow to spark df:: 1.1740 s\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 1000)\n",
    "#pandas_df = timed(\"With Arrow to pandas_df:\", lambda:spark_df.toPandas())\n",
    "spark_df = timed(\"With Arrow to spark df:\", lambda:spark.createDataFrame(pandas_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567f1d6-a058-4416-a1fc-0381fdce4a02",
   "metadata": {},
   "source": [
    "### Pandas <-> Polars: conversion runtime comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00ace64b-6fec-4b5f-a587-6a731126ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Arrow to polars df:: 0.1204 s\n",
      "Without Arrow to pandas df:: 0.2680 s\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n",
    "polars_df = timed(\"Without Arrow to polars df:\", lambda: pl.from_pandas(pandas_df))\n",
    "pandas_df = timed(\"Without Arrow to pandas df:\", lambda: polars_df.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c5fff6d-7070-4994-8d7b-6c509f5fc278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Arrow to polars df:: 0.0490 s\n",
      "Without Arrow to pandas df:: 0.0590 s\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "polars_df = timed(\"Without Arrow to polars df:\", lambda: pl.from_pandas(pandas_df))\n",
    "pandas_df = timed(\"Without Arrow to pandas df:\", lambda: polars_df.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18995dcb-aa5c-4b74-9014-908f3c3bf25d",
   "metadata": {},
   "source": [
    "### Spark <-> Polars: conversion runtime comparison\n",
    "#### intermediate step `toPandas` needed --> use PySpark Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "149cc813-8d07-404e-8f1c-fe598ab9ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can implement the conversion steps here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f5a41-4a4a-4808-97b8-a423f4837cb5",
   "metadata": {},
   "source": [
    "# `read`, `fillna` and `groupBy` Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c76ac5-e7e8-41e4-8f9f-4dc80e7cd6f5",
   "metadata": {},
   "source": [
    "## Pandas: Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440cb09-86d7-4f53-b4f4-bf196bcf5465",
   "metadata": {},
   "source": [
    "### Duration of Pandas `read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49d4ce47-0f2c-4b9f-8b2a-8c39536b643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas read: 2.7614 s\n"
     ]
    }
   ],
   "source": [
    "pandas_df = timed(\"Pandas read\", lambda:pd.read_csv(\"large_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccbe44fb-dcc8-4f0d-acc0-16ca16915e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pandas ---\n",
      "pandas_fillna: 0.0559 s\n",
      "pandas_groupby: 0.1506 s\n",
      "category\n",
      "1    100.995639\n",
      "2    102.010160\n",
      "3    103.013624\n",
      "4    103.974664\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#df = pd.DataFrame({'id': id, 'x': x, 'category': y, 'target': target})\n",
    "\n",
    "print(\"--- pandas ---\")\n",
    "pandas_df = timed('pandas_fillna', lambda: pandas_df.fillna(0))\n",
    "pandas_df_group = timed('pandas_groupby', lambda: pandas_df.groupby('category')['target'].mean())\n",
    "print(pandas_df_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9fc2f-43b9-4387-908c-c091dbfe7a94",
   "metadata": {},
   "source": [
    "## Polars: Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2f2576d-fe4c-42d0-b2bb-26089ef15cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_groupby(polars_df):\n",
    "    return polars_df.select([\n",
    "        pl.col(\"category\"),\n",
    "        pl.col(\"target\").mean().over(\"category\").alias(\"mean_target\")\n",
    "    ]).unique(subset=\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7325a5-4aa0-4c70-b0a0-2672950e543d",
   "metadata": {},
   "source": [
    "### Duration of Polars `read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e91f6050-5151-470c-a3e7-3c70e2b382ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars read: 0.5195 s\n"
     ]
    }
   ],
   "source": [
    "polars_df = timed(\"Polars read\", lambda:pl.read_csv(\"large_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a0d0ad1-1092-4742-8687-060f25071805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- polars ---\n",
      "polars_fillna: 0.1026 s\n",
      "polars_groupby: 0.2109 s\n",
      "shape: (4, 2)\n",
      "┌──────────┬─────────────┐\n",
      "│ category ┆ mean_target │\n",
      "│ ---      ┆ ---         │\n",
      "│ i64      ┆ f64         │\n",
      "╞══════════╪═════════════╡\n",
      "│ 3        ┆ 103.013624  │\n",
      "│ 4        ┆ 103.974664  │\n",
      "│ 1        ┆ 100.995639  │\n",
      "│ 2        ┆ 102.01016   │\n",
      "└──────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"--- polars ---\")\n",
    "polars_df = timed('polars_fillna', lambda: polars_df.fill_null(0))\n",
    "polars_df_group = timed('polars_groupby', lambda:polars_groupby(polars_df))\n",
    "print(polars_df_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9034b-7269-4c50-afa8-83f69c20ca95",
   "metadata": {},
   "source": [
    "## PySpark: Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13788441-d635-471a-b9d3-a265e25710a7",
   "metadata": {},
   "source": [
    "### Duration of Spark `read.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d0fcd57-0a10-4cc4-8212-2dec0c8ed447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark read: 18.3132 s\n"
     ]
    }
   ],
   "source": [
    "spark_df = timed(\"Spark read\", lambda: spark.read.csv(\"large_dataset.csv\", header=True, inferSchema=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69d946c2-6186-40da-84be-321cd04573e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pyspark ---\n",
      "spark_fillna: 0.1062 s\n",
      "spark_groupby: 0.1753 s\n",
      "+--------+------------------+\n",
      "|category|              mean|\n",
      "+--------+------------------+\n",
      "|       1|100.99563886783933|\n",
      "|       3|103.01362398979641|\n",
      "|       4|103.97466350434654|\n",
      "|       2|102.01016019808266|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- pyspark ---\")\n",
    "spark = SparkSession.builder.master('local[*]').appName('SimpleCompare').getOrCreate()\n",
    "spark_df = timed('spark_fillna', lambda: spark_df.na.fill(0))\n",
    "spark_df_group = timed('spark_groupby', lambda: spark_df.groupBy('category').agg(F.mean('target').alias('mean')))\n",
    "spark_df_group.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640e180-3f33-4fee-9799-ec468b91fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
