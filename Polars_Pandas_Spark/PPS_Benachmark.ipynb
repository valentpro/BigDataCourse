{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7435b4-1d24-4f7d-82de-162199d05ea2",
   "metadata": {},
   "source": [
    "# Arrow, Pandas, Polars and PySpark - simple Comparison\n",
    "In this notebook, we want to take a closer look at the concepts and learn which one is best suited for which scenario.\n",
    "Credits:\n",
    "- https://amanjaiswalofficial.medium.com/apache-arrow-making-spark-even-faster-3ae-8ca8e1a67dc7\n",
    "- https://www.datacamp.com/de/tutorial/apache-arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc48ca4-9383-4c6d-8839-596b4f46e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import time\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "RUN_SPARK = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b265923-dccc-4822-b2d7-ae185d953161",
   "metadata": {},
   "source": [
    "## Generate Test Data\n",
    "### lets first generate some test data\n",
    "you can always adjust the size by adjusting `NUM_ROWS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb1769b-5e97-44e1-a328-50be380b6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 5000000 # kann angepasst werden\n",
    "\n",
    "# %%\n",
    "# Datensatz erzeugen\n",
    "np.random.seed(42)\n",
    "id = np.arange(1, NUM_ROWS+1)\n",
    "x = np.random.randn(NUM_ROWS) * 10 + 50\n",
    "y = np.random.randint(1, 5, size=NUM_ROWS)\n",
    "target = 2*x + y + np.random.randn(NUM_ROWS)*5\n",
    "\n",
    "df_pd = pd.DataFrame({\n",
    "    'id': id,\n",
    "    'x': x,\n",
    "    'category': y,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "df_pd.to_csv('large_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58ea892-4b50-4cb9-ad2c-c1b1bea713e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer\n",
    "def timed(name, func):\n",
    "    start = perf_counter()\n",
    "    result = func()\n",
    "    end = perf_counter()\n",
    "    print(f\"{name}: {end-start:.4f} s\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a214d-573c-43cb-ba38-d96cea3dca63",
   "metadata": {},
   "source": [
    "## Arrow\n",
    "\n",
    "\"Apache Arrow is a multi-language toolbox for building high performance applications that process and transport large data sets. It is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another.\n",
    "\n",
    "A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more\". [https://arrow.apache.org/overview/]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e67979af-2ad5-40d5-bbd5-a5ed9e801251",
   "metadata": {},
   "source": [
    "### Why Arrow\n",
    "- Arrow uses its in-memory format, which has memory buffers organized in columns and batches. \n",
    "- this makes vectorised processing possible, performing operations on entire columns efficiently\n",
    "\n",
    "### Shared Memory Model\n",
    "- Arrow avoids traditional serialization\n",
    "- Instead, it relies on a shared memory model in which multiple processes can directly access the same data without copying or converting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ba7e7-e315-4c94-9cc0-81f8d8b7d974",
   "metadata": {},
   "source": [
    "### `to_pandas` runtime comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7925ad60-baaa-4969-a5c9-4eb171952fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Runtime_Comparison\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n",
    "df = spark.read.csv(\"large_dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec8e1cfb-760f-419a-b9b5-c588c8b0096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Arrow to pdf:: 58.6861 s\n",
      "Without Arrow to spark df:: 168.0553 s\n"
     ]
    }
   ],
   "source": [
    "#pdf = timed(\"Without Arrow to pdf:\", lambda:df.toPandas())\n",
    "spark_df = timed(\"Without Arrow to spark df:\", lambda:spark.createDataFrame(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3c61293-9311-40c5-a6e0-3552e426ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Arrow to spark df:: 5.2246 s\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 1000)\n",
    "#pdf = timed(\"With Arrow to pdf:\", lambda:df.toPandas())\n",
    "spark_df = timed(\"With Arrow to spark df:\", lambda:spark.createDataFrame(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55f50f-14a0-4725-b7bd-87b02199c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark_df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567f1d6-a058-4416-a1fc-0381fdce4a02",
   "metadata": {},
   "source": [
    "### Pandas Polars conversion runtime comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00ace64b-6fec-4b5f-a587-6a731126ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Arrow to polars df:: 172.0041 s\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n",
    "pl_df_arrow = timed(\"Without Arrow to polars df:\", lambda:spark.createDataFrame(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c5fff6d-7070-4994-8d7b-6c509f5fc278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Arrow to polars df:: 5.7771 s\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "pl_df_arrow = timed(\"Without Arrow to polars df:\", lambda:spark.createDataFrame(pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18995dcb-aa5c-4b74-9014-908f3c3bf25d",
   "metadata": {},
   "source": [
    "### PySpark Polars conversion runtime comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39fa82-5dc5-4f3f-a434-cbdcb9d2274b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n",
    "arrow_table = spark_df._collect_as_arrow()  # intern Arrow Table\n",
    "pl_df_arrow = timed(\"Without Arrow to polars df:\", lambda:spark_df._collect_as_arrow())\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "pl_df_arrow = timed(\"With Arrow to polars df:\", lambda:spark_df._collect_as_arrow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f5a41-4a4a-4808-97b8-a423f4837cb5",
   "metadata": {},
   "source": [
    "# `fillna` and `groupBy` Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c76ac5-e7e8-41e4-8f9f-4dc80e7cd6f5",
   "metadata": {},
   "source": [
    "## Pandas: Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccbe44fb-dcc8-4f0d-acc0-16ca16915e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pandas ---\n",
      "pandas_fillna: 0.2920 s\n",
      "pandas_groupby: 0.2242 s\n",
      "category\n",
      "1    100.988890\n",
      "2    101.991171\n",
      "3    103.010719\n",
      "4    103.985833\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#df = pd.DataFrame({'id': id, 'x': x, 'category': y, 'target': target})\n",
    "\n",
    "print(\"--- pandas ---\")\n",
    "pdf = timed('pandas_fillna', lambda: pdf.fillna(0))\n",
    "df_group = timed('pandas_groupby', lambda: pdf.groupby('category')['target'].mean())\n",
    "print(df_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9fc2f-43b9-4387-908c-c091dbfe7a94",
   "metadata": {},
   "source": [
    "## Polars: Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a0d0ad1-1092-4742-8687-060f25071805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- polars ---\n",
      "polars_fillna: 0.0401 s\n",
      "polars_groupby: 3.7003 s\n",
      "shape: (4, 2)\n",
      "┌──────────┬─────────────┐\n",
      "│ category ┆ mean_target │\n",
      "│ ---      ┆ ---         │\n",
      "│ i32      ┆ f64         │\n",
      "╞══════════╪═════════════╡\n",
      "│ 3        ┆ 103.001336  │\n",
      "│ 4        ┆ 103.990736  │\n",
      "│ 1        ┆ 101.00083   │\n",
      "│ 2        ┆ 101.997864  │\n",
      "└──────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "def polars_groupby():\n",
    "    return df_pl.select([\n",
    "        pl.col(\"category\"),\n",
    "        pl.col(\"target\").mean().over(\"category\").alias(\"mean_target\")\n",
    "    ]).unique(subset=\"category\")\n",
    "\n",
    "print(\"--- polars ---\")\n",
    "df_pl = df_pl = pl.from_pandas(df)\n",
    "df_pd = timed('polars_fillna', lambda: df_pl.fill_null(0))\n",
    "df_group_pl = timed('polars_groupby', polars_groupby)\n",
    "print(df_group_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9034b-7269-4c50-afa8-83f69c20ca95",
   "metadata": {},
   "source": [
    "## PySpark: Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69d946c2-6186-40da-84be-321cd04573e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pyspark ---\n",
      "spark_from_pandas: 186.8315 s\n",
      "spark_fillna: 0.0650 s\n",
      "spark_groupby: 0.1378 s\n",
      "+--------+------------------+\n",
      "|category|              mean|\n",
      "+--------+------------------+\n",
      "|       1|100.98889018639353|\n",
      "|       3|103.01071857029602|\n",
      "|       2|101.99117119627803|\n",
      "|       4|103.98583261511732|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pd = pd.DataFrame({\n",
    "    'id': id,\n",
    "    'x': x,\n",
    "    'category': y,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "print(\"--- pyspark ---\")\n",
    "spark = SparkSession.builder.master('local[*]').appName('SimpleCompare').getOrCreate()\n",
    "sdf = timed('spark_from_pandas', lambda: spark.createDataFrame(df))\n",
    "sdf = timed('spark_fillna', lambda: sdf.na.fill(0))\n",
    "sdf_group = timed('spark_groupby', lambda: sdf.groupBy('category').agg(F.mean('target').alias('mean')))\n",
    "sdf_group.show(5)\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
